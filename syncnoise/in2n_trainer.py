# Copyright 2022 The Nerfstudio Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Code to train model, only needed in order to not save InstructPix2Pix checkpoints

File copied from https://github.com/ayaanzhaque/instruct-nerf2nerf/tree/main/in2n
"""
from dataclasses import dataclass, field
from typing import Type
from rich.progress import Console
import torch
from nerfstudio.engine.trainer import Trainer, TrainerConfig
from nerfstudio.viewer.viewer_elements import ViewerButton
from nerfstudio.utils.decorators import check_main_thread

CONSOLE = Console(width=120)


@dataclass
class InstructNeRF2NeRFTrainerConfig(TrainerConfig):
    """Configuration for the InstructNeRF2NeRFTrainer."""
    _target: Type = field(default_factory=lambda: InstructNeRF2NeRFTrainer)


class InstructNeRF2NeRFTrainer(Trainer):
    """Trainer for InstructNeRF2NeRF"""

    def __init__(self, config: TrainerConfig, local_rank: int = 0, world_size: int = 1) -> None:

        super().__init__(config, local_rank, world_size)

        # reset button
        self.reset_button = ViewerButton(name="Reset Button", cb_hook=self.reset_callback)

    def reset_callback(self, handle: ViewerButton) -> None:
        """Reset the model to the original checkpoint"""
        
        # load checkpoint
        self._load_checkpoint()

        # reset step
        self.pipeline.starting_step = None

        # reset dataset
        self.pipeline.datamanager.image_batch_updated.fill_(False)
        self.pipeline.datamanager.only_sample_updated = False
        self.pipeline.datamanager.image_batch['image'] = self.pipeline.datamanager.original_image_batch['image'].clone()
        self.pipeline.datamanager.image_batch['image_idx'] = self.pipeline.datamanager.original_image_batch['image_idx'].clone()
        CONSOLE.print(f'reset with prompt {self.pipeline.config.prompt} '
                      f'text guidance scale {self.pipeline.config.guidance_scale} '
                      f'image guidance scale {self.pipeline.config.image_guidance_scale}')

    @check_main_thread
    def save_checkpoint(self, step: int) -> None:
        """Save the model and optimizers
        Args:
            step: number of steps in training for given checkpoint
        """
        # possibly make the checkpoint directory
        if not self.checkpoint_dir.exists():
            self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        # save the checkpoint
        ckpt_path = self.checkpoint_dir / f"step-{step:09d}.ckpt"
        pipeline_state_dict = {k: v for k, v in self.pipeline.state_dict().items() if "ip2p." not in k}
        torch.save(
            {
                "step": step,
                "pipeline": self.pipeline.module.state_dict()  # type: ignore
                if hasattr(self.pipeline, "module")
                else pipeline_state_dict,
                "optimizers": {k: v.state_dict() for (k, v) in self.optimizers.optimizers.items()},
                "scalers": self.grad_scaler.state_dict(),
            },
            ckpt_path,
        )
        # possibly delete old checkpoints
        if self.config.save_only_latest_checkpoint:
            # delete everything else in the checkpoint folder
            for f in self.checkpoint_dir.glob("*"):
                if f != ckpt_path:
                    f.unlink()
